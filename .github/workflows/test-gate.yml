name: Test Gate
on:
  workflow_dispatch:
  pull_request:
    branches: ["main", "dev"]
    types: [opened, synchronize, reopened]

jobs:
  detect-changes:
    name: Detect Code Changes
    runs-on: ubuntu-latest
    outputs:
      should-test: ${{ steps.filter.outputs.code }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            code:
              - '**.py'
              - '**/requirements*.txt'
              - 'pyproject.toml'

  unit-tests:
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.should-test == 'true' }}
    name: Unit Tests
    runs-on: ubuntu-latest
    permissions:
      contents: read
    outputs:
      main_total: ${{ steps.results.outputs.main_total }}
      main_passed: ${{ steps.results.outputs.main_passed }}
      main_skipped: ${{ steps.results.outputs.main_skipped }}
      main_failed: ${{ steps.results.outputs.main_failed }}
      media_total: ${{ steps.results.outputs.media_total }}
      media_passed: ${{ steps.results.outputs.media_passed }}
      media_skipped: ${{ steps.results.outputs.media_skipped }}
      media_failed: ${{ steps.results.outputs.media_failed }}
      tests_failed: ${{ steps.results.outputs.tests_failed }}
    strategy:
      matrix:
        python-version: ["3.10"]
    env:
      PYTHONPATH: ${{ github.workspace }}:${{ github.workspace }}/tt-media-server

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
          cache-dependency-path: 'requirements-dev.txt'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install -r tt-media-server/requirements.txt

      - name: Run tt-inference-server tests
        run: |
          echo "Running tests from tests/ directory..."
          pytest tests/ --ignore=tests/server_tests -v --tb=short --continue-on-collection-errors > main_test_output.txt 2>&1 || echo "MAIN_PYTEST_EXIT_NONZERO=true" >> $GITHUB_ENV

          # try to find final pytest summary line (robust)
          SUMMARY_LINE=$(grep -E "==+ .* in [0-9]+(\.[0-9]+)?s" main_test_output.txt | tail -n 1 || true)
          if [ -z "$SUMMARY_LINE" ]; then
            SUMMARY_LINE=$(tail -n 5 main_test_output.txt | sed -n '/./,$p' | tail -n 1 || true)
          fi

          extract_number() {
            line="$1"; token="$2"
            echo "$line" | sed -nE "s/.* ([0-9]+) ${token}.*/\\1/p" || echo "0"
          }

          MAIN_PASSED=$(extract_number "$SUMMARY_LINE" "passed")
          MAIN_FAILED=$(extract_number "$SUMMARY_LINE" "failed")
          MAIN_SKIPPED=$(extract_number "$SUMMARY_LINE" "skipped")
          MAIN_ERRORS=$(echo "$SUMMARY_LINE" | sed -nE 's/.* ([0-9]+) errors.*/\1/p' || true)
          if [ -z "$MAIN_ERRORS" ]; then
            MAIN_ERRORS=$(echo "$SUMMARY_LINE" | sed -nE 's/.* ([0-9]+) error.*/\1/p' || true)
          fi
          MAIN_ERRORS=${MAIN_ERRORS:-0}
          MAIN_COLLECTED=$(extract_number "$SUMMARY_LINE" "collected")

          # Try to detect collection-time errors like "X tests collected / Y errors"
          MAIN_COLLECTION_ERRORS=$(
            grep -oE "[0-9]+ tests collected / [0-9]+ errors" main_test_output.txt \
              | sed -nE 's/.*\/ ([0-9]+) errors/\1/p' \
              | tail -n1 || echo "0"
          )
          MAIN_COLLECTION_ERRORS=${MAIN_COLLECTION_ERRORS:-0}

          # sanitize numeric-only
          MAIN_PASSED=$(echo "$MAIN_PASSED" | tr -dc '0-9'); MAIN_PASSED=${MAIN_PASSED:-0}
          MAIN_FAILED=$(echo "$MAIN_FAILED" | tr -dc '0-9'); MAIN_FAILED=${MAIN_FAILED:-0}
          MAIN_SKIPPED=$(echo "$MAIN_SKIPPED" | tr -dc '0-9'); MAIN_SKIPPED=${MAIN_SKIPPED:-0}
          MAIN_ERRORS=$(echo "$MAIN_ERRORS" | tr -dc '0-9'); MAIN_ERRORS=${MAIN_ERRORS:-0}
          MAIN_COLLECTED=$(echo "$MAIN_COLLECTED" | tr -dc '0-9'); MAIN_COLLECTED=${MAIN_COLLECTED:-0}
          MAIN_COLLECTION_ERRORS=$(echo "$MAIN_COLLECTION_ERRORS" | tr -dc '0-9'); MAIN_COLLECTION_ERRORS=${MAIN_COLLECTION_ERRORS:-0}

          echo "MAIN_PASSED=$MAIN_PASSED" >> $GITHUB_ENV
          echo "MAIN_FAILED=$MAIN_FAILED" >> $GITHUB_ENV
          echo "MAIN_SKIPPED=$MAIN_SKIPPED" >> $GITHUB_ENV
          echo "MAIN_ERRORS=$MAIN_ERRORS" >> $GITHUB_ENV
          echo "MAIN_COLLECTED=$MAIN_COLLECTED" >> $GITHUB_ENV
          echo "MAIN_COLLECTION_ERRORS=$MAIN_COLLECTION_ERRORS" >> $GITHUB_ENV

          cat main_test_output.txt

          # Fail this step if there's at least 1 failed test OR at least 1 error (including collection errors)
          MAIN_TOTAL_FAILED=$(( MAIN_FAILED + MAIN_ERRORS + MAIN_COLLECTION_ERRORS ))
          if [ "$MAIN_TOTAL_FAILED" -gt 0 ] || [ "${MAIN_PYTEST_EXIT_NONZERO:-}" = "true" ]; then
            echo "MAIN_TESTS_FAILED=true" >> $GITHUB_ENV
            # fail the step intentionally so PR will be blocked
            exit 1
          fi

      - name: Run tt-media-server tests
        if: always()
        env:
          HF_TOKEN: ""
          MODEL: ""
          JWT_SECRET: ""
        working-directory: tt-media-server
        run: |
          echo "Running tests from tt-media-server/tests/"
          pytest tests/ -v --tb=short --continue-on-collection-errors > ../media_test_output.txt 2>&1 || echo "MEDIA_PYTEST_EXIT_NONZERO=true" >> $GITHUB_ENV

          # parse final pytest summary line robustly
          SUMMARY_LINE=$(grep -E "==+ .* in [0-9]+(\.[0-9]+)?s" ../media_test_output.txt | tail -n 1 || true)
          if [ -z "$SUMMARY_LINE" ]; then
            SUMMARY_LINE=$(tail -n 5 ../media_test_output.txt | sed -n '/./,$p' | tail -n 1 || true)
          fi

          extract_number() {
            line="$1"; token="$2"
            echo "$line" | sed -nE "s/.* ([0-9]+) ${token}.*/\\1/p" || echo "0"
          }

          MEDIA_PASSED=$(extract_number "$SUMMARY_LINE" "passed")
          MEDIA_FAILED=$(extract_number "$SUMMARY_LINE" "failed")
          MEDIA_SKIPPED=$(extract_number "$SUMMARY_LINE" "skipped")
          MEDIA_ERRORS=$(echo "$SUMMARY_LINE" | sed -nE 's/.* ([0-9]+) errors.*/\1/p' || true)
          if [ -z "$MEDIA_ERRORS" ]; then
            MEDIA_ERRORS=$(echo "$SUMMARY_LINE" | sed -nE 's/.* ([0-9]+) error.*/\1/p' || true)
          fi
          MEDIA_ERRORS=${MEDIA_ERRORS:-0}
          MEDIA_COLLECTED=$(extract_number "$SUMMARY_LINE" "collected")

          # Try to detect collection-time errors like "X tests collected / Y errors"
          MEDIA_COLLECTION_ERRORS=$(
            grep -oE "[0-9]+ tests collected / [0-9]+ errors" ../media_test_output.txt \
              | sed -nE 's/.*\/ ([0-9]+) errors/\1/p' \
              | tail -n1 || echo "0"
          )
          MEDIA_COLLECTION_ERRORS=${MEDIA_COLLECTION_ERRORS:-0}

          # sanitize numeric-only
          MEDIA_PASSED=$(echo "$MEDIA_PASSED" | tr -dc '0-9'); MEDIA_PASSED=${MEDIA_PASSED:-0}
          MEDIA_FAILED=$(echo "$MEDIA_FAILED" | tr -dc '0-9'); MEDIA_FAILED=${MEDIA_FAILED:-0}
          MEDIA_SKIPPED=$(echo "$MEDIA_SKIPPED" | tr -dc '0-9'); MEDIA_SKIPPED=${MEDIA_SKIPPED:-0}
          MEDIA_ERRORS=$(echo "$MEDIA_ERRORS" | tr -dc '0-9'); MEDIA_ERRORS=${MEDIA_ERRORS:-0}
          MEDIA_COLLECTED=$(echo "$MEDIA_COLLECTED" | tr -dc '0-9'); MEDIA_COLLECTED=${MEDIA_COLLECTED:-0}
          MEDIA_COLLECTION_ERRORS=$(echo "$MEDIA_COLLECTION_ERRORS" | tr -dc '0-9'); MEDIA_COLLECTION_ERRORS=${MEDIA_COLLECTION_ERRORS:-0}

          # persist numeric envs (safe)
          echo "MEDIA_PASSED=$MEDIA_PASSED" >> $GITHUB_ENV
          echo "MEDIA_FAILED=$MEDIA_FAILED" >> $GITHUB_ENV
          echo "MEDIA_SKIPPED=$MEDIA_SKIPPED" >> $GITHUB_ENV
          echo "MEDIA_ERRORS=$MEDIA_ERRORS" >> $GITHUB_ENV
          echo "MEDIA_COLLECTED=$MEDIA_COLLECTED" >> $GITHUB_ENV
          echo "MEDIA_COLLECTION_ERRORS=$MEDIA_COLLECTION_ERRORS" >> $GITHUB_ENV

          cat ../media_test_output.txt

          # Track failures but don't exit yet - let summary run
          MEDIA_TOTAL_FAILED=$(( MEDIA_FAILED + MEDIA_ERRORS + MEDIA_COLLECTION_ERRORS ))
          echo "MEDIA_TOTAL_FAILED=$MEDIA_TOTAL_FAILED" >> $GITHUB_ENV
          if [ "$MEDIA_TOTAL_FAILED" -gt 0 ] || [ "${MEDIA_PYTEST_EXIT_NONZERO:-}" = "true" ]; then
            echo "MEDIA_TESTS_FAILED=true" >> $GITHUB_ENV
            echo "‚ö†Ô∏è tt-media-server tests had failures, but continuing to summary..."
          fi

      - name: Compute Test Results
        id: results
        if: always()
        run: |
          # --- Re-parse both outputs to guarantee correct numbers (avoid env contamination) ---
          parse_summary() {
            file="$1"
            # find final summary line
            sline=$(grep -E "==+ .* in [0-9]+(\.[0-9]+)?s" "$file" | tail -n1 || true)
            if [ -z "$sline" ]; then
              sline=$(tail -n 5 "$file" | sed -n '/./,$p' | tail -n 1 || true)
            fi
            extract() {
              echo "$sline" | sed -nE "s/.* ([0-9]+) ${1}.*/\\1/p" || echo "0"
            }
            passed=$(extract "passed"); passed=$(echo "$passed" | tr -dc '0-9'); passed=${passed:-0}
            failed=$(extract "failed"); failed=$(echo "$failed" | tr -dc '0-9'); failed=${failed:-0}
            skipped=$(extract "skipped"); skipped=$(echo "$skipped" | tr -dc '0-9'); skipped=${skipped:-0}
            errors=$(echo "$sline" | sed -nE 's/.* ([0-9]+) errors.*/\1/p' || true)
            if [ -z "$errors" ]; then
              errors=$(echo "$sline" | sed -nE 's/.* ([0-9]+) error.*/\1/p' || true)
            fi
            errors=$(echo "$errors" | tr -dc '0-9'); errors=${errors:-0}
            collected=$(extract "collected"); collected=$(echo "$collected" | tr -dc '0-9'); collected=${collected:-0}
            collection_errors=$(
              grep -oE "[0-9]+ tests collected / [0-9]+ errors" "$file" \
                | sed -nE 's/.*\/ ([0-9]+) errors/\1/p' \
                | tail -n1 || echo "0"
            )
            collection_errors=$(echo "$collection_errors" | tr -dc '0-9'); collection_errors=${collection_errors:-0}

            # compute totals: total = passed + skipped + failed + errors + collection_errors
            total_failed=$(( failed + errors + collection_errors ))
            total=$(( passed + skipped + failed + errors + collection_errors ))

            printf "%s|%s|%s|%s|%s|%s|%s\n" "$passed" "$skipped" "$failed" "$errors" "$collection_errors" "$total_failed" "$total"
          }

          # parse MAIN
          MAIN_VALUES=$(parse_summary main_test_output.txt)
          MAIN_PASSED=$(echo "$MAIN_VALUES" | cut -d'|' -f1)
          MAIN_SKIPPED=$(echo "$MAIN_VALUES" | cut -d'|' -f2)
          MAIN_TOTAL_FAILED=$(echo "$MAIN_VALUES" | cut -d'|' -f6)
          MAIN_TOTAL=$(echo "$MAIN_VALUES" | cut -d'|' -f7)

          # parse MEDIA
          MEDIA_VALUES=$(parse_summary media_test_output.txt)
          MEDIA_PASSED=$(echo "$MEDIA_VALUES" | cut -d'|' -f1)
          MEDIA_SKIPPED=$(echo "$MEDIA_VALUES" | cut -d'|' -f2)
          MEDIA_TOTAL_FAILED=$(echo "$MEDIA_VALUES" | cut -d'|' -f6)
          MEDIA_TOTAL=$(echo "$MEDIA_VALUES" | cut -d'|' -f7)

          # Set outputs for downstream jobs
          echo "main_total=$MAIN_TOTAL" >> $GITHUB_OUTPUT
          echo "main_passed=$MAIN_PASSED" >> $GITHUB_OUTPUT
          echo "main_skipped=$MAIN_SKIPPED" >> $GITHUB_OUTPUT
          echo "main_failed=$MAIN_TOTAL_FAILED" >> $GITHUB_OUTPUT
          echo "media_total=$MEDIA_TOTAL" >> $GITHUB_OUTPUT
          echo "media_passed=$MEDIA_PASSED" >> $GITHUB_OUTPUT
          echo "media_skipped=$MEDIA_SKIPPED" >> $GITHUB_OUTPUT
          echo "media_failed=$MEDIA_TOTAL_FAILED" >> $GITHUB_OUTPUT

          # Determine if tests failed
          if [ "$MAIN_TOTAL_FAILED" -gt 0 ] || [ "$MEDIA_TOTAL_FAILED" -gt 0 ]; then
            echo "tests_failed=true" >> $GITHUB_OUTPUT
          else
            echo "tests_failed=false" >> $GITHUB_OUTPUT
          fi

          # Print summary to console
          echo "===== üß™ Unit Test Results ====="
          echo "tt-inference-server: $MAIN_PASSED passed, $MAIN_SKIPPED skipped, $MAIN_TOTAL_FAILED failed (total: $MAIN_TOTAL)"
          echo "tt-media-server: $MEDIA_PASSED passed, $MEDIA_SKIPPED skipped, $MEDIA_TOTAL_FAILED failed (total: $MEDIA_TOTAL)"

          # Fail if tests failed
          if [ "$MAIN_TOTAL_FAILED" -gt 0 ] || [ "$MEDIA_TOTAL_FAILED" -gt 0 ]; then
            exit 1
          fi


  llm-streaming-performance:
    name: LLM Streaming Performance Test
    runs-on: ubuntu-latest
    permissions:
      contents: read
    outputs:
      status: ${{ steps.streaming_test.outputs.status }}
    env:
      PYTHONPATH: ${{ github.workspace }}/tt-media-server
      HF_TOKEN: ""
      MODEL: ""
      JWT_SECRET: ""
    defaults:
      run:
        working-directory: tt-media-server

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: 'pip'
          cache-dependency-path: 'tt-media-server/requirements.txt'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run LLM Streaming Performance Test
        id: streaming_test
        run: |
          set +e
          pytest performance_tests/test_llm_streaming.py -sv 2>&1 | tee streaming_test_output.txt
          PYTEST_EXIT=${PIPESTATUS[0]}
          set -e

          if [ "$PYTEST_EXIT" = "0" ]; then
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "status=failed" >> $GITHUB_OUTPUT
          fi

          exit $PYTEST_EXIT

      - name: Upload streaming test output
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: streaming-test-output
          path: tt-media-server/streaming_test_output.txt
          retention-days: 7


  test-summary:
    name: Test Results Summary
    needs: [unit-tests, llm-streaming-performance]
    if: always()
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Download streaming test output
        if: always()
        uses: actions/download-artifact@v4
        with:
          name: streaming-test-output
          path: artifacts
        continue-on-error: true

      - name: Generate Combined Summary
        if: always()
        run: |
          # Get unit test results from job outputs
          MAIN_TOTAL="${{ needs.unit-tests.outputs.main_total }}"
          MAIN_PASSED="${{ needs.unit-tests.outputs.main_passed }}"
          MAIN_SKIPPED="${{ needs.unit-tests.outputs.main_skipped }}"
          MAIN_FAILED="${{ needs.unit-tests.outputs.main_failed }}"
          MEDIA_TOTAL="${{ needs.unit-tests.outputs.media_total }}"
          MEDIA_PASSED="${{ needs.unit-tests.outputs.media_passed }}"
          MEDIA_SKIPPED="${{ needs.unit-tests.outputs.media_skipped }}"
          MEDIA_FAILED="${{ needs.unit-tests.outputs.media_failed }}"
          UNIT_TESTS_FAILED="${{ needs.unit-tests.outputs.tests_failed }}"

          # Handle skipped unit-tests job
          if [ -z "$MAIN_TOTAL" ]; then
            MAIN_TOTAL="0"; MAIN_PASSED="0"; MAIN_SKIPPED="0"; MAIN_FAILED="0"
            MEDIA_TOTAL="0"; MEDIA_PASSED="0"; MEDIA_SKIPPED="0"; MEDIA_FAILED="0"
            UNIT_TESTS_FAILED="skipped"
          fi

          # Get streaming test status
          STREAMING_STATUS="${{ needs.llm-streaming-performance.outputs.status }}"
          STREAMING_JOB_RESULT="${{ needs.llm-streaming-performance.result }}"

          # Calculate totals
          TOTAL_TESTS=$(( MAIN_TOTAL + MEDIA_TOTAL ))
          TOTAL_PASSED=$(( MAIN_PASSED + MEDIA_PASSED ))
          TOTAL_SKIPPED=$(( MAIN_SKIPPED + MEDIA_SKIPPED ))
          TOTAL_FAILED=$(( MAIN_FAILED + MEDIA_FAILED ))

          # Determine statuses
          MAIN_STATUS="‚úÖ"; [ "$MAIN_FAILED" -gt 0 ] && MAIN_STATUS="‚ùå"
          MEDIA_STATUS="‚úÖ"; [ "$MEDIA_FAILED" -gt 0 ] && MEDIA_STATUS="‚ùå"
          OVERALL_STATUS="‚úÖ"; [ "$TOTAL_FAILED" -gt 0 ] && OVERALL_STATUS="‚ùå"

          if [ "$STREAMING_STATUS" = "passed" ]; then
            STREAMING_ICON="‚úÖ"
          elif [ "$STREAMING_JOB_RESULT" = "skipped" ]; then
            STREAMING_ICON="‚è≠Ô∏è"
            STREAMING_STATUS="skipped"
          else
            STREAMING_ICON="‚ùå"
            STREAMING_STATUS="failed"
          fi

          # Write summary header
          {
            echo "# üß™ Test Results Summary"
            echo ""
            echo "## Unit Tests"
            echo ""
            echo "| Component | Total | Passed | Skipped | Failed | Status |"
            echo "|-----------|------:|-------:|--------:|-------:|:------:|"
            echo "| **tt-inference-server** | $MAIN_TOTAL | $MAIN_PASSED | $MAIN_SKIPPED | $MAIN_FAILED | $MAIN_STATUS |"
            echo "| **tt-media-server** | $MEDIA_TOTAL | $MEDIA_PASSED | $MEDIA_SKIPPED | $MEDIA_FAILED | $MEDIA_STATUS |"
            echo "| **Overall** | **$TOTAL_TESTS** | **$TOTAL_PASSED** | **$TOTAL_SKIPPED** | **$TOTAL_FAILED** | $OVERALL_STATUS |"
            echo ""
            echo "## üöÄ LLM Streaming Performance Test"
            echo ""
            echo "**Status**: $STREAMING_ICON $STREAMING_STATUS"
            echo ""
          } >> $GITHUB_STEP_SUMMARY

          # Append raw streaming test output if available
          if [ -f "artifacts/streaming_test_output.txt" ]; then
            {
              echo "<details>"
              echo "<summary>üìã Test Output (click to expand)</summary>"
              echo ""
              echo '```'
              cat artifacts/streaming_test_output.txt
              echo '```'
              echo ""
              echo "</details>"
            } >> $GITHUB_STEP_SUMMARY
          else
            echo "_Streaming test output not available_" >> $GITHUB_STEP_SUMMARY
          fi

          # Final status
          {
            echo ""
            echo "---"
            if [ "$TOTAL_FAILED" -gt 0 ] || [ "$STREAMING_STATUS" = "failed" ]; then
              echo "‚ùå **Some tests failed - PR merge will be blocked**"
            else
              echo "‚úÖ **All tests passed - PR is ready for review**"
            fi
          } >> $GITHUB_STEP_SUMMARY

          # Store for PR comment
          echo "MAIN_TOTAL=$MAIN_TOTAL" >> $GITHUB_ENV
          echo "MAIN_PASSED=$MAIN_PASSED" >> $GITHUB_ENV
          echo "MAIN_SKIPPED=$MAIN_SKIPPED" >> $GITHUB_ENV
          echo "MAIN_FAILED=$MAIN_FAILED" >> $GITHUB_ENV
          echo "MEDIA_TOTAL=$MEDIA_TOTAL" >> $GITHUB_ENV
          echo "MEDIA_PASSED=$MEDIA_PASSED" >> $GITHUB_ENV
          echo "MEDIA_SKIPPED=$MEDIA_SKIPPED" >> $GITHUB_ENV
          echo "MEDIA_FAILED=$MEDIA_FAILED" >> $GITHUB_ENV
          echo "TOTAL_TESTS=$TOTAL_TESTS" >> $GITHUB_ENV
          echo "TOTAL_PASSED=$TOTAL_PASSED" >> $GITHUB_ENV
          echo "TOTAL_SKIPPED=$TOTAL_SKIPPED" >> $GITHUB_ENV
          echo "TOTAL_FAILED=$TOTAL_FAILED" >> $GITHUB_ENV
          echo "STREAMING_STATUS=$STREAMING_STATUS" >> $GITHUB_ENV

      - name: Comment on PR
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const { MAIN_TOTAL, MAIN_PASSED, MAIN_SKIPPED, MAIN_FAILED,
                    MEDIA_TOTAL, MEDIA_PASSED, MEDIA_SKIPPED, MEDIA_FAILED,
                    TOTAL_TESTS, TOTAL_PASSED, TOTAL_SKIPPED, TOTAL_FAILED,
                    STREAMING_STATUS } = process.env;

            const unitTestsPassed = parseInt(TOTAL_FAILED) === 0;
            const streamingPassed = STREAMING_STATUS === 'passed' || STREAMING_STATUS === 'skipped';
            const overallPassed = unitTestsPassed && streamingPassed;
            const statusIcon = overallPassed ? '‚úÖ' : '‚ùå';
            const statusText = overallPassed ? 'PASSED' : 'FAILED';

            const streamingIcon = STREAMING_STATUS === 'passed' ? '‚úÖ' :
                                  STREAMING_STATUS === 'skipped' ? '‚è≠Ô∏è' : '‚ùå';

            const commentBody = `## ${statusIcon} Test Results - ${statusText}

            ### Unit Tests
            | Component | Total | Passed | Skipped | Failed | Status |
            |-----------|-------|--------|---------|--------|--------|
            | **tt-inference-server** | ${MAIN_TOTAL} | ${MAIN_PASSED} | ${MAIN_SKIPPED} | ${MAIN_FAILED} | ${parseInt(MAIN_FAILED) === 0 ? '‚úÖ' : '‚ùå'} |
            | **tt-media-server** | ${MEDIA_TOTAL} | ${MEDIA_PASSED} | ${MEDIA_SKIPPED} | ${MEDIA_FAILED} | ${parseInt(MEDIA_FAILED) === 0 ? '‚úÖ' : '‚ùå'} |
            | **Overall** | ${TOTAL_TESTS} | ${TOTAL_PASSED} | ${TOTAL_SKIPPED} | ${TOTAL_FAILED} | ${unitTestsPassed ? '‚úÖ' : '‚ùå'} |

            ### LLM Streaming Performance
            **Status**: ${streamingIcon} ${STREAMING_STATUS}

            ### Details
            - **Python Version**: 3.10
            - **Workflow**: \`${context.workflow}\`
            - **Commit**: \`${context.sha.substring(0, 7)}\`
            - **Run ID**: [${context.runId}](${context.payload.repository.html_url}/actions/runs/${context.runId})

            ${overallPassed
              ? 'üéâ All tests passed! This PR is ready for review.'
              : '‚ö†Ô∏è Some tests failed. Please review the failing tests before merging.'}`;

            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.data.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Test Results -')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: commentBody
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: commentBody
              });
            }


  forge-runner-changes:
    name: Detect Forge Runner Changes
    runs-on: ubuntu-latest
    outputs:
      should-run: ${{ steps.filter.outputs.forge_runner }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Evaluate forge runner paths
        id: filter
        if: ${{ github.event_name == 'pull_request' }}
        uses: dorny/paths-filter@v3
        with:
          filters: |
            forge_runner:
              - 'tt-media-server/requirements.txt'
              - 'tt-media-server/tt_model_runners/**'


  forge-runner-cpu-tests:
    needs: forge-runner-changes
    if: ${{ needs.forge-runner-changes.outputs.should-run == 'true' }}
    name: Forge Runner CPU Tests (Python 3.11)
    runs-on: ubuntu-latest
    permissions:
      contents: read
    defaults:
      run:
        working-directory: tt-media-server

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Free Disk Space (Ubuntu)
        uses: jlumbroso/free-disk-space@main
        with:
          tool-cache: false
          android: true
          dotnet: true
          haskell: true
          large-packages: true
          docker-images: true
          swap-storage: true

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          # Dissable pip caching for now, it take longer than install sometimes
          # cache: 'pip'
          # cache-dependency-path: |
          #   tt-media-server/requirements.txt
          #   tt-media-server/tt_model_runners/forge_runners/requirements.txt

      - name: Create venv, install dependencies
        shell: bash
        run: |
          python3.11 -m venv venv-3.11
          source venv-3.11/bin/activate
          export VLLM_TARGET_DEVICE="empty"
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r tt_model_runners/forge_runners/requirements.txt
          pip install pytest pytest-asyncio

      - name: Run CPU-only tests
        shell: bash
        run: |
          source venv-3.11/bin/activate
          pytest tt_model_runners/forge_runners/test_forge_models.py -k "cpu" -v

  ci-gate:
    name: CI Gate
    runs-on: ubuntu-latest
    needs: [unit-tests, llm-streaming-performance, forge-runner-cpu-tests]
    if: always()
    steps:
      - name: Check results
        run: |
          # Fail if any required job failed
          if [[ "${{ needs.unit-tests.result }}" == "failure" ]]; then
            echo "‚ùå Unit tests failed"
            exit 1
          fi
          if [[ "${{ needs.llm-streaming-performance.result }}" == "failure" ]]; then
            echo "‚ùå LLM streaming performance tests failed"
            exit 1
          fi
          if [[ "${{ needs.forge-runner-cpu-tests.result }}" == "failure" ]]; then
            echo "‚ùå Forge runner tests failed"
            exit 1
          fi
          echo "‚úÖ All checks passed (or were skipped)"

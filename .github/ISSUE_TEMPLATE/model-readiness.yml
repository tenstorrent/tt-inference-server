name: Add a Model to Model Readiness
description: Add support for a new model in the Model Readiness suite.
title: "[Model Readiness Support]: "
labels: ["model_readiness_support"]
projects: ["tenstorrent/130"]
body:
  - type: input
    id: model-name
    attributes:
      label: Model Name
      description: What is the exact name of the model?
    validations:
      required: true
  - type: input
    id: model-url
    attributes:
      label: Model URL
      description: Provide a link to the model publisher's repository, weights download, or documentation.
      placeholder: e.g. https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct
    validations:
      required: true
  - type: input
    id: model-demo-code
    attributes:
      label: Model Tenstorrent code implementation
      description: Provide a link to the Tenstorrent model implementation demo code.
      placeholder: e.g. https://github.com/tenstorrent/tt-metal/tree/v0.60.0-rc22/models/tt_transformers
    validations:
      required: true
  - type: textarea
    id: model-eval-tasks
    attributes:
      label: What accuracy evals are needed? # This is the question
      description: Corresponds to the evals/eval_config.py task_name.
      placeholder: e.g. r1_aime24, leaderboard_ifeval, meta_ifeval, etc. ...
    validations:
      required: true
  - type: checkboxes
    id: model-readiness-checklist # Unique ID for this checklist
    attributes:
      label: Model Readiness Support Checklist
      description: These are the required tasks to add support for a new model. This is independent of the model implementation passing criteria for performance or accuracy, e.g. Customer Functional / Customer Complete / Top Perf..
      options:
        - label: Tenstorrent code implementation has all runtime preconditions and configurations documented.
          required: false
        - label: vLLM or other inference server integration complete.
          required: false
        - label: Performance targets (Top Perf) set in benchmarking/benchmark_targets/model_performance_reference.json.
          required: false
        - label: Required evals are implemented and integrated for model in evals/eval_config.py.
          required: false
        - label: GPU accuracy reference scores collected and set for model in evals/eval_config.py.
          required: false
        - label: Model is added to Model Readiness workflow code, inference server can serve model and run release workflow.
          required: false
        - label: Model Readiness release markdown report for target hardware pasted in comment below.
          required: false
        - label: Documentation - Model is added to README table, specific inference server docs covers how to serve model and run release workflow.
          required: false
        - label: Model is added to tt-shield Model Readiness CI nightly run producing release report data.
          required: false

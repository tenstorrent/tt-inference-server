diff --git a/benchmarks/backend_request_func.py b/benchmarks/backend_request_func.py
index 4813fde2..0cb3e72e 100644
--- a/benchmarks/backend_request_func.py
+++ b/benchmarks/backend_request_func.py
@@ -235,9 +235,7 @@ async def async_request_openai_completions(
             "model": request_func_input.model,
             "prompt": request_func_input.prompt,
             "temperature": 0.0,
-            "best_of": request_func_input.best_of,
             "max_tokens": request_func_input.output_len,
-            "logprobs": request_func_input.logprobs,
             "stream": True,
             "ignore_eos": request_func_input.ignore_eos,
         }
diff --git a/benchmarks/benchmark_serving.py b/benchmarks/benchmark_serving.py
index c1a396c8..74f75a15 100644
--- a/benchmarks/benchmark_serving.py
+++ b/benchmarks/benchmark_serving.py
@@ -22,6 +22,12 @@ On the client side, run:
         --endpoint /generate_stream
     to the end of the command above.
 """
+import sys
+from unittest.mock import MagicMock
+# mock out ttnn fully so we can import ttnn without using it
+sys.modules["ttnn"] = MagicMock()
+sys.modules["ttnn.device"] = MagicMock()
+
 import argparse
 import asyncio
 import base64
@@ -417,7 +423,7 @@ async def benchmark(
         prompt_len=test_prompt_len,
         output_len=test_output_len,
         logprobs=logprobs,
-        best_of=best_of,
+        best_of=None,
         multi_modal_content=test_mm_content,
         ignore_eos=ignore_eos,
     )

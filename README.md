# TT-Inference-Server

Tenstorrent Inference Server (`tt-inference-server`) is the repo of available model APIs for deploying on Tenstorrent hardware.

## Official Repository

[https://github.com/tenstorrent/tt-inference-server](https://github.com/tenstorrent/tt-inference-server/)


## Getting Started
Please follow setup instructions found in each model folder's README.md doc

--------------------------------------------------------------------------------------------------------------

## Model Implementations
| Model          | Hardware                    |
|----------------|-----------------------------|
| [Qwen 2.5 72B](vllm-tt-metal-llama3/README.md)   | [TT-QuietBox & TT-LoudBox](https://tenstorrent.com/hardware/tt-quietbox)    |
| [LLaMa 3.3 70B](vllm-tt-metal-llama3/README.md)  | [TT-QuietBox & TT-LoudBox](https://tenstorrent.com/hardware/tt-quietbox)    |
| [LLaMa 3.2 11B Vision](vllm-tt-metal-llama3/README.md)  | [n300](https://tenstorrent.com/hardware/wormhole) |
| [LLaMa 3.2 3B](vllm-tt-metal-llama3/README.md)  | [n150](https://tenstorrent.com/hardware/wormhole) |
| [LLaMa 3.2 1B](vllm-tt-metal-llama3/README.md)  | [n150](https://tenstorrent.com/hardware/wormhole) |
| [LLaMa 3.1 70B](vllm-tt-metal-llama3/README.md)  | [TT-QuietBox & TT-LoudBox](https://tenstorrent.com/hardware/tt-quietbox) |
| [LLaMa 3.1 8B](vllm-tt-metal-llama3/README.md)  | [n150](https://tenstorrent.com/hardware/wormhole)    |

# TT-Inference-Server

Tenstorrent Inference Server (`tt-inference-server`) is the repo of available model APIs for deploying on Tenstorrent hardware.

## Official Repository

[https://github.com/tenstorrent/tt-inference-server](https://github.com/tenstorrent/tt-inference-server/)


## Getting Started
Please follow setup instructions found in each model folder's README.md doc

--------------------------------------------------------------------------------------------------------------

## Model Implementations
| Model          | Hardware                    |
|----------------|-----------------------------|
| [LLaMa 3.1 70B](vllm-tt-metal-llama3-70b/README.md)  | TT-QuietBox & TT-LoudBox    |
| [Mistral 7B](tt-metal-mistral-7b/README.md) | n150 and n300|
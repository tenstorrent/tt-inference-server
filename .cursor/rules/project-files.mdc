---
description: 
globs: 
alwaysApply: true
---
---
description: Prioritize key project directories and files
globs:
  - tests/**
  - workflows/**
  - evals/**
  - benchmarking/**
  - utils/**
  - tt-metal-sdxl/**
  - run.py
alwaysApply: true
---

- These are the key files in the project (output from `tree -L`)
benchmarking
├── benchmark_config.py
├── benchmark_targets
├── benchmark-uplift.patch
├── benchmark_vllm_offline_llama31_70b.sh
├── manual_benchmarks.md
├── manual_run_benchmarks.py
├── prompt_client_online_benchmark.py
├── README.md
├── run_benchmarks.py
├── summary_report.py
└── vllm_online_benchmark.py
docker-entrypoint.sh
docs
├── development.md
├── git-workflow.png
└── workflows_user_guide.md
evals
├── eval_config.py
├── eval_utils.py
├── manual_evals.md
├── out.json
├── README.md
├── run_evals_cpu.sh
├── run_evals.py
├── run_evals.sh
├── run_evals_vision.sh
├── setup_evals_meta.sh
└── setup_evals.sh
LICENSE
locust
├── data_reader.py
├── locust_config.conf
├── locustfile.py
├── README.md
└── requirements.txt
pyproject.toml
README.md
requirements-dev.txt
run.py
scripts
└── add_spdx_header.py
setup.sh
tests
├── benchmark_vllm_offline_inference.py
├── mock_benchmark_vllm_offline_inference.py
├── mock_vllm_api_server.py
├── mock_vllm_model.py
├── mock_vllm_offline_inference_tt.py
├── mock.vllm.openai.api.dockerfile
├── README.md
├── run_vllm_seq_lens.py
├── test_model_spec.py
├── test_run_arguments.py
└── utils
tt-metal-sdxl
├── config
│   ├── constants.py
│   └── settings.py
├── Dockerfile
├── Dockerfile.forge
├── domain
│   ├── audio_transcription_request.py
│   ├── base_request.py
│   ├── image_generate_request.py
│   └── image_search_request.py
├── evals
├── load_tester.py
├── main.py
├── model_services
│   ├── audio_service.py
│   ├── base_service.py
│   ├── cnn_service.py
│   ├── device_worker.py
│   ├── image_service.py
│   └── scheduler.py
├── open_ai_api
│   ├── __init__.py
│   ├── audio.py
│   ├── cnn.py
│   ├── image.py
│   └── tt_maintenance_api.py
├── pyproject.toml
├── README.md
├── requirements.txt
├── resolver
│   ├── __init__.py
│   ├── scheduler_resolver.py
│   ├── service_resolver_test.py
│   └── service_resolver.py
├── run_uvicorn.sh
├── scripts
│   └── download_sdxl_weights.py
├── security
│   └── api_key_cheker.py
├── static
│   └── demos
│       └── resnet.html
├── tests
│   ├── api_test.py
│   ├── conftest.py
│   ├── test_device_worker.py
│   ├── test_forge_runner.py
│   ├── test_main.py
│   ├── test_runner_fabric.py
│   ├── test_scheduler.py
│   ├── test_sd35_device_runner.py
│   ├── test_users.py
│   └── whisper_demo.py
├── tt_model_runners
│   ├── base_device_runner.py
│   ├── conftest.py
│   ├── forge_runners
│   │   ├── base.py
│   │   ├── config.py
│   │   ├── forge_runner.py
│   │   ├── imagenet_class_index.json
│   │   ├── imagenet_class_list.txt
│   │   ├── loader.py
│   │   ├── README.md
│   │   ├── requirements.txt
│   │   ├── run_clean_env.py
│   │   └── setup_env.sh
│   ├── mock_runner.py
│   ├── resources
│   │   └── coco.names
│   ├── runner_fabric.py
│   ├── sd35_runner.py
│   ├── sdxl_runner_trace.py
│   ├── whisper_runner.py
│   └── yolov4_runner.py
└── utils
    ├── audio_manager.py
    ├── helpers.py
    ├── image_manager.py
    └── logger.py
utils
├── batch_processor.py
├── capture_traces.py
├── __init__.py
├── logging_utils.py
├── prompt_client_cli.py
├── prompt_client.py
├── prompt_configs.py
├── prompt_generation.py
├── prompt_templates
└── README.md
VERSION
vllm-tt-metal-llama3
├── docs
├── README.md
├── requirements.txt
├── src
├── vllm.tt-metal.src.cloud.Dockerfile
└── vllm.tt-metal.src.dev.Dockerfile
workflows
├── build_docker.sh
├── build_release_docker_images.py
├── __init__.py
├── log_setup.py
├── model_spec.py
├── README.md
├── release_docs.py
├── release.py
├── run_docker_server.py
├── run_reports.py
├── run_workflows.py
├── setup_host.py
├── utils.py
├── workflow_config.py
├── workflow_types.py
└── workflow_venvs.py

